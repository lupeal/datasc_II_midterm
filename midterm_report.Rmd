---
title: "P8106 Midterm Project: Predicting COVID-19 Recovery Time"
author: 
  - "Guadalupe Antonio Lopez, Gustavo Garcia-Franceschini, Derek Lamb"
  -  "UNI's: GA2612, GEG2145, DRL2168"
header-includes:
    - \usepackage{setspace}\doublespacing
    - \usepackage[font={small}]{caption}

output: pdf_document
---

```{r setup, include=FALSE, message = FALSE}
library(tidyverse)
library(tidymodels)
library(caret)
library(rsample)
library(corrplot)
library(gtsummary)
library(mgcv)
library(gridExtra)

theme_set(theme_bw() + theme(legend.position = "bottom"))

knitr::opts_chunk$set(
  echo = TRUE, 
  fig.width = 5,
  out.width = "65%",
  fig.align = "center")
```

## Introduction and Exploratory Data Analysis

This analysis combines three cohort studies regarding recovery time from COVID-19 illness. We have the individual's gender and race, along with other medical information. Among these, stand out their vaccination status and the study (A or B) they were a part of. With this information, we aim to fit a model that can both help us predict recovery time, and understand variables strongly associated with increased risk for long COVID-19 recovery times.

To start our investigation, we conduct exploratory data analysis. To explore our data and train the models, we partitioned the data into training and testing sets, with 80% of the data (2400 subjects) being assigned to the training set, and the remaining 20% (600 subjects) being assigned to the test set. This way, the test set is not included in our EDA. The split was done with a random seed of 1.

```{r load data, echo = FALSE}
load("files/recovery.RData")
df_rec <- dat |> 
  select(-id)

set.seed(1)
dat_split = initial_split(data = df_rec, prop = 0.8)

df_train = training(dat_split)
df_test = testing(dat_split)
```

We calculated summary statistics for our variables, grouping them by study group,  in **Table 1**. We noticed that they are very similar in all our proposed covariates, yet the mean recovery time for individuals in gorup B is five days more than for individuals in group A. It is also important to note that group A has almost twice as many individuals as group B.

```{r, echo = FALSE, message = FALSE}
df_train |> 
  mutate(
    race = case_match(race,
      "1" ~ "White",
      "2" ~ "Asian",
      "3" ~ "Black",
      "4" ~ "Hispanic"),
    smoking = case_match(smoking,
      "0" ~ "never",
      "1" ~ "current",
      "2" ~ "former")
  ) |> 
  rename("sex (male)" = gender) |> 
  tbl_summary(by = study,
              statistic = list(all_continuous() ~ '{mean} ({sd})',
                             all_categorical() ~ '{n} ({p}%)'),
              digits = all_continuous() ~ 3) %>%
  modify_spanning_header(c('stat_1', 'stat_2') ~ '**Study**') %>%
  modify_header(label = '**Variable**') %>%
  bold_labels() %>%
  modify_caption('Summary statistics')
```


We further investigate the relationship between study group and recovery time in **Figure 1A**. We found that the COVID-19 infection recovery time is heavily right-skewed, regardless of the study group. However, Study A has a later peak, while Study B has a heavier tail, corresponding to more individuals in that study experiencing longer recovery time. This is an early indication that study group might be an important variable when predicting recovery time. We decided to include the study variable in the training process, and allow CV procedures to determine whether it is a useful predictor of recovery time.

In **Figure 1B**, we plotted some relevant variables on the x-axis, with recovery time on the y-axis. We see that bmi has some non-linearity, while weight and LDL show some outliers in the middle of their ranges. These observations suggest we should implement models that account for non-linearity. Our categorical variables plotted have similar recovery time distributions.

```{r combined plot, echo=FALSE, fig.cap="Relationship between covariates and recovery time. A. Density of recovery time by study. B. Scatterplots of continuous variables against recovery time", fig.width=10, fig.height=4, out.width="90%"}
# density plot
dens_plot <- df_train |> 
  ggplot() +
  geom_density(aes(x = recovery_time, col = study)) +
  theme_bw() + 
  labs(
       x = "Recovery time (days)",
       y = "Density",
       color = "Study") +
  ggtitle(bquote(bold("A")))

# feature plot
x <- model.matrix(recovery_time ~ ., df_train)[, -1]
y <- df_train$recovery_time

feat_plot <- featurePlot(x[, c(1, 8:10, 13:14)], y, plot = 'scatter', 
          labels = c('', 'recovery time'), type = c('p'), 
          main=list(label = "B", cex = 1.1, x=0.2, y=0.2))

# combine them
grid.arrange(dens_plot, feat_plot, ncol=2)
```


We also examined the pairwise correlations of the variables, and the correlations of the covariates with the recovery time. There were two clusters of strong correlation (height, weight, and BMI; hypertension and SBP), but these covariates were functionally dependent upon each other. There were no other strong correlations between variables, and no one covariate had an exceptional correlation to recovery time.

```{r corrplot, echo=FALSE, fig.cap="Variable correlation plot"}
# convert covariates to numeric
cor_rec <- df_train |> 
  model.matrix(recovery_time ~ ., data = _) 

# put outcome back into matrix
cor_rec[,1] = df_train$recovery_time
colnames(cor_rec)[1] <- "recovery"

# create corrplot
cor_rec |> 
  cor() |> 
  corrplot()
```


## Model Training

To predict COVID-19 recovery time, we modeled the data using four approaches -- two linear and two non-linear. For the linear approaches, we selected elastic net and partial least squares regression. For the nonlinear approaches, we selected multivariate adaptive regression splines (MARS) and a general additive model (GAM). As specified before, we use our training set to train all models.

All models were fitted using the `train()` function in the `caret` package. Although some inputs varied by model, the common inputs were formula or model matrix and response vector, data, method, tuning parameters grid, and a 10-fold cross validation method. We used a common seed (1) to partition the data and train all models.


```{r set cv splits, echo = FALSE}
set.seed(1)

#matrix models
x_training <- model.matrix(recovery_time ~ ., df_train)[, -1]
y_training <- df_train$recovery_time

x_testing <- model.matrix(recovery_time ~ ., df_test)[, -1]
y_testing <- df_test$recovery_time

#10foldCV
ctrl1 <- trainControl(method = 'cv', number = 10)
```

### Elastic Net Model

```{r enet, echo = FALSE}
#elastic net
set.seed(1)

enet_fit <- train(recovery_time ~ .,
                  data = df_train,
                  method = 'glmnet',
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21),
                                         lambda = exp(seq(-6, 1, length = 100))),
                  trControl = ctrl1)
```

To fit the elastic net model, we used the model formula with `recovery_time` as the response and all other variables in our training data set to be predictors. We fit models with tuning parameter alpha to be evenly spaced between 0 and 1 (with length 21) and lambda to be exponentially spaced between -6 and 1 (with length 100). We settled on this lambda region after fitting the model various times with different regions. We started with a large region (-4 to 4), but realized that our preferred lambda value was close to our lower boundary. Thus, we continued to expand our region until we settled on -6 to 1.

```{r, include = FALSE}
coef(enet_fit$finalModel, enet_fit$bestTune$lambda)
```

After fitting the elastic net model, the final model based on the optimal lambda contained 17 predictors and an intercept -- no predictors were shrunk to zero. The values for each predictor represent the estimated effect of each predictor on recovery time. Based on our output, age, height, bmi, and systolic blood pressure had positive coefficients. This suggests that an increase in a given predictor is associated with an increase in recovery time. There were also positive coefficients for categorical variables race (asian), former and current smoking status, hypertension, systolic blood pressure, severity, and study B. These positive coefficients indicate the difference in the outcome compared to their reference level. 

### Partial Least Squares (PLS) Regression Model

```{r pls, echo = FALSE}
#PLS
set.seed(1)

pls_fit <- train(x_training, y_training,
                 method = 'pls',
                 tuneGrid = data.frame(ncomp = 1:15),
                 trControl = ctrl1,
                 preProcess = c('center', 'scale'))
```

To fit the PLS model, we used the training model matrix, based on our training data, and training response vector. When tuning the model, we examined a number of components ranging between 1 and 15. This range is based on the number of variables in our training model matrix. Additionally, the predictor data was centered and scaled.


```{r, include = FALSE}
coef(pls_fit$finalModel)
```

After fitting the PLS model, the final model was based on 13 components. The positive predictor coefficients in the elastic net model were also positive in the PLS model. In this case, the positive coefficients suggest that an increase in a predictor variable (by one standard deviation) is associated with an increase in standardized recovery time by the coefficient value (with standard deviation units).

### Multivariate Adaptive Regression Splines Model

```{r mars, echo = FALSE, message = FALSE}
#MARS
set.seed(1)

mars_fit <- train(x_training, y_training,
                  method = 'earth',
                  tuneGrid = expand.grid(degree = 1:3,
                                         nprune = 2:14),
                  trControl = ctrl1)
```

To fit the MARS model, we used the training model matrix and training response vector. For tuning the MARS model, we examined degrees ranging between 1 and 3 and the maximum number of terms in the pruned model to range between 2 and 14.

```{r, include = FALSE}
coef(mars_fit$finalModel)
```

After fitting the MARS model, the final model is based on 12 terms including an intercept based on 9 predictors. The predictors that most inform recovery time are bmi, height, weight, vaccination status, study group B, LDL, systolic blood pressure, COVID-19 severity status, and smoking status.

### General Additive Model

```{r gam, echo = FALSE, message = FALSE}
#GAM
set.seed(1)

gam_fit <- train(x_training, y_training,
                 method = 'gam',
                 trControl = ctrl1)
```

To fit the GAM, we used the training model matrix and training response vector. We used generalized cross-validation with Mallow's Cp to select the smoothing parameter, and considered both "True" and "False" selection.

```{r, include = FALSE}
gam_fit$finalModel
```

The final GAM contains categorical predictors gender, race (all levels), current smoking status, hypertension, diabetes, vaccination status, COVID-19 severity status, study group B, and smooth terms applied to continuous variables age, systolic blood pressure, LDL, bmi, height, weight. The model has a total estimated degrees of freedom of 35.99, which suggests a relatively flexible model. The model that performed best had the select tuning parameter as "False".

## Results 

Using the approaches above, we had four models that were trained on the same partitions of data, one selected from Elastic Net, PLS, MARS, and GAM. We then compared these four models by resampling on the training set. In the figure below, we constructed boxplots to compare the four different models by their resampled RMSE. The two linear models and GAM perform about the same, though GAM was a bit better on average. MARS noticeably outperformed the other models.

```{r resamples, echo=FALSE, fig.cap="Resampled RMSE for our four models"}
rs = resamples(list(
  ElasticNet = enet_fit,
  PLS = pls_fit,
  MARS = mars_fit,
  GAM = gam_fit
))

bwplot(rs, metric = "RMSE")
```


```{r final test error, echo=FALSE}
test_pred <- predict(mars_fit, newdata = x_testing) 
test_rmse <- sqrt(mean((test_pred - y_testing)^2))
```

Our MARS model takes form:

$\hat{Y_i} = 18.408 + h(31.1-bmi_i) \times 3.604 + h(164-height_i) \times h(bmi_i-31.1) \times studyB_i \times 3.146 + h(weight_i-89.3) \times h(bmi_i-31.1) \times studyB_i \times -4.935 + h(bmi_i-25.8) \times 6.272 + vaccine_i \times -5.833 +  h(bmi_i-31.1) \times h(LDL_i-84) \times studyB_i \times 4.451 + h(bmi_i-31.1) \times h(LDL_i-79) \times studyB_i \times -4.209 + h(bmi_i-25.8) \times h(141-SBP_i) \times -0.076 + h(weight_i-78.7) \times h(bmi_i-31.1) \times studyB_i \times 2.877 + severity_i \times studyB_i \times 14.321 + smoking1_i \times h(bmi_i-31.1) \times studyB_i \times 9.162$

We see the association between study B and recovery time in this model. For example, for bmi greater than 31.1, the association between study B and recovery time is positive if the height is less than 164, but negative if the weight is above 89.3. We are unable to get a sense of the overall association between study B and recovery time using MARS, which is one of the downsides of the model. We can, however, say that the study group is clearly important in predicting recovery time, since it is included in many of the terms of the model. The test RMSE for the MARS model is `r test_rmse |> round(digits=2)`.

## Conclusion

In this project, our goal was to use statistical learning to gain insight into the recovery process of people infected with COVID-19. We fit four models to predict COVID-19 recovery time from a set of 14 covariates, two linear and two nonlinear. Our linear models achieved similar performance in predicting recovery time, but they were outdone by the nonlinear methods, MARS in particular. This improvement in prediction is due to the greater flexibility of the nonlinear methods, but comes at a trade-off of the interpretability of such models. However, as our goal was to develop the best model for predicting COVID-19 recovery time, we are comfortable giving up some of this interpretability, and recommending the MARS model developed above for this task.

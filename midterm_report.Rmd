---
title: "P8106 Midterm Project: Predicting COVID-19 Recovery Time"
author: 
  - "Guadalupe Antonio Lopez, Gustavo Garcia-Franceschini, Derek Lamb"
  -  "UNI's: GA2612, GEG2145, DRL2168"
header-includes:
    - \usepackage{setspace}\doublespacing

output: pdf_document
---

```{r setup, include=FALSE, message = FALSE}
library(tidyverse)
library(tidymodels)
library(caret)
library(rsample)
library(corrplot)
library(gtsummary)
library(mgcv)

theme_set(theme_bw() + theme(legend.position = "bottom"))

knitr::opts_chunk$set(
  echo = TRUE, 
  fig.width = 6,
  out.width = "65%",
  fig.align = "center")
```

## Introduction

This analysis combines three cohort studies regarding recovery time from COVID-19 illness. We have the individual's gender and race, along with other medical information. Among these, stand out their vaccination status and the study (A or B) they were a part of. With this information, we aim to fit a model that can both help us predict recovery time, and help us understand variables strongly associated with increased risk for long COVID-19 recovery times.

## EDA


```{r, echo = FALSE}
load("files/recovery.RData")
df_rec <- dat |> 
  select(-id)

set.seed(1)
dat_split = initial_split(data = df_rec, prop = 0.8)

df_train = training(dat_split)
df_test = testing(dat_split)
```


```{r, echo = FALSE, message = FALSE}
df_rec %>%
  tbl_summary(by = study,
              statistic = list(all_continuous() ~ '{mean} ({sd})',
                             all_categorical() ~ '{n} ({p}%)'),
              digits = all_continuous() ~ 3) %>%
  modify_spanning_header(c('stat_1', 'stat_2') ~ '**Study**') %>%
  modify_header(label = '**Variable**') %>%
  bold_labels() %>%
  modify_caption('Summary statistics')
```



**The table probably goes first.**

We found that the COVID-19 infection recovery time is heavily right-skewed: most of the individuals recovered at around 6 weeks, but there are individuals that only recovered from the infection after three months (or more) of being infected. This may mean that we'll need flexible models to capture the skewness of the response.

```{r histogram of recovery time, echo=FALSE}
ggplot(df_train) +
  geom_histogram(aes(x = recovery_time), binwidth = 3, fill = "gold2") +
  labs(title = "COVID-19 recovery time is right-skewed",
       x = "Recovery time (days)",
       y = "Count")
```



```{r, echo = FALSE}
x <- model.matrix(recovery_time ~ ., df_rec)[, -1]
y <- df_rec$recovery_time

featurePlot(x[, c(2, 9:11, 14,15)], y, plot = 'scatter', 
            labels = c('', 'Y'), type = c('p'))
```

**And also the scatterplots with continuous variables**



When evaluating the distribution of recovery time, split into study groups, we find that its different for the two distributions. Study A has a later peak, while Study B has a heavier tail, corresponding to more individuals in that study experiencing longer recovery time. This is an early indication that study group might be an important variable when predicting recovery time. 

```{r density plot by study, echo=FALSE}
df_train %>%
  ggplot() +
  geom_density(aes(x = recovery_time, col = study)) +
  theme_bw() + 
  labs(title = "The two studies have different recovery time distributions",
       x = "Recovery time (days)",
       y = "Density",
       color = "Study")
```




We also examined the pairwise correlations of the variables, and the correlations of the covariates with the recovery time. There were two clusters of strong correlation (height, weight, and BMI; hypertension and SBP), but these covariates were functionally dependent upon each other. There were no other strong correlations between variables, and no one covariate had an exceptional correlation to recovery time.

```{r corrplot, echo=FALSE}
# convert covariates to numeric
cor_rec <- df_rec |> 
  model.matrix(recovery_time ~ ., data = _) 

# put outcome back into matrix
cor_rec[,1] = df_rec$recovery_time
colnames(cor_rec)[1] <- "recovery"

# create corrplot
cor_rec |> 
  cor() |> 
  corrplot()
```
**Figure M.** Correlogram of study variables.





## Model Training

To train the models, we partitioned the data into training and testing sets, with 80% of the data (2400 subjects) being assigned to the training set, and the remaining 20% (600 subjects) being assigned to the test set.

To predict COVID-19 recovery time, we modeled the data using four approaches -- two linear and two non-linear. For the linear approaches, we selected elastic net and partial least squares regression. For the nonlinear approaches, we selected multivariate adaptive regression splines (MARS) and a general additive model (GAM). 

All models were fitted using the `train()` function in the `caret` package. Although some inputs varied by model, the common inputs were formula or model matrix and response vector, data, method, tuning parameters grid, and a 10-fold cross validation method. 

**Note that all models were fit using a seed of 1.**


```{r set cv splits, echo = FALSE}
set.seed(1)

#matrix models
x_training <- model.matrix(recovery_time ~ ., df_train)[, -1]
y_training <- df_train$recovery_time

x_testing <- model.matrix(recovery_time ~ ., df_test)[, -1]
y_testing <- df_test$recovery_time

#10foldCV
ctrl1 <- trainControl(method = 'cv', number = 10)
```

### Elastic Net Model

```{r enet, echo = FALSE}
#elastic net
set.seed(1)

enet_fit <- train(recovery_time ~ .,
                  data = df_train,
                  method = 'glmnet',
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21),
                                         lambda = exp(seq(-6, 1, length = 100))),
                  trControl = ctrl1)
```

To fit the elastic net model, we used the model formula with `recovery_time` as the response and all other variables in our training data set to be predictors. Given that we fit an elastic net model, the method specified was `glmnet`, with tuning parameter alpha to be sequenced between 0 and 1 (with length 21) and lambda to be exponentially sequenced between -6 and 1 (with length 100). We settled on this lambda region after fitting the model various times with different regions. We started with a large region (-4 to 4), but realized that our preferred lambda value was close to our lower boundary. Thus, we continued to expand our region until we settled on -6 to 1.


```{r, include = FALSE}
coef(enet_fit$finalModel, enet_fit$bestTune$lambda)
```

After fitting the elastic net model, the final model based on the optimal lambda contained 17 predictors and an intercept -- no predictors were shrunk to zero. The values for each predictor represent the estimated effect of each predictor on recovery time. Based on our output, age, height, bmi, and systolic blood pressure had positive coefficients. This suggests that an increase in a given predictor is associated with an increase in recovery time. There were also positive coefficients for categorical variables race (asian), former and current smoking status, hypertension, systolic blood pressure, severity, and study B. These positive coefficients indicate the difference in the outcome compared to their reference level. 




### Partial Least Squares (PLS) Regression Model

```{r pls, echo = FALSE}
#PLS
set.seed(1)

pls_fit <- train(x_training, y_training,
                 method = 'pls',
                 tuneGrid = data.frame(ncomp = 1:15),
                 trControl = ctrl1,
                 preProcess = c('center', 'scale'))
```

To fit the PLS model, we used the training model matrix, based on our training data, and training response vector. Given that we fit a PLS model, the method specified was `pls`, with number of components ranging between 1 and 15. This range is based on the number of variables in our training model matrix. Additionally, the predictor data was centered and scaled.


```{r, include = FALSE}
coef(pls_fit$finalModel)
```

After fitting the PLS model, the final model is based on 13 components.





### Multivariate Adaptive Regression Splines Model

```{r mars, echo = FALSE, message = FALSE}
#MARS
set.seed(1)

mars_fit <- train(x_training, y_training,
                  method = 'earth',
                  tuneGrid = expand.grid(degree = 1:3,
                                         nprune = 2:14),
                  trControl = ctrl1)
```

To fit the MARS model, we used the training model matrix and training response vector. Given that we fit a MARS model, the method specified was `earth`, with degrees ranging between 1 and 3 and the maximum number of terms in the pruned model to range between 2 and 14.




### General Additive Model

```{r gam, echo = FALSE, message = FALSE}
#GAM
set.seed(1)

gam_fit <- train(x_training, y_training,
                 method = 'gam',
                 trControl = ctrl1)
```

To fit the GAM, we used the training model matrix and training response vector. Given that we fit a GAM, the method specified was `gam` and a 10-fold cross validation.






## Results 


We then compared the four models that we fit by resampling on the training set. In the figure below, we constructed boxplots to compare the four different models by their resampled RMSE. The two linear models and GAM perform about the same, though GAM was a bit better on average. MARS noticeably outperformed the other models.

```{r resamples, echo=FALSE}
rs = resamples(list(
  ElasticNet = enet_fit,
  PLS = pls_fit,
  MARS = mars_fit,
  GAM = gam_fit
))

bwplot(rs, metric = "RMSE")
```

Once we decided on MARS as a final model, we calculated the test error using the 20% partition of the initial data set.
```{r final test error, echo=FALSE}
test_pred <- predict(mars_fit, newdata = x_testing) 
test_rmse <- sqrt(mean((test_pred - y_testing)^2))
```
The test RMSE for the MARS model is `r test_rmse |> round(digits=2)`.


## Conclusion

In this project, our goal was to use statistical learning to gain insight into the recovery process of people infected with COVID-19. We fit four models to predict COVID-19 recovery time from a set of 14 covariates, two linear and two nonlinear. Our linear models achieved similar performance in predicting recovery time, but they were outdone by the nonlinear methods, MARS in particular. This improvement in prediction is due to the greater flexibility of the nonlinear methods, but comes at a trade-off of the interpretability of such models. However, as our goal was to develop the best model for predicting COVID-19 recovery time, we are comfortable giving up some of this interpretability, and recommending the MARS model developed above for this task.
